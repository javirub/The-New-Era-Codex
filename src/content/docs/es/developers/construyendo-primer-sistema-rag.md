---
title: "Construyendo tu Primer Sistema RAG con LangChain"
description: "Tutorial paso a paso para crear un RAG b√°sico con embeddings, ChromaDB y OpenAI"
sidebar:
  order: 40
  badge:
    text: "Intermedio"
    variant: note
version: "1.0"
---

# Construyendo tu Primer Sistema RAG con LangChain

## Descripci√≥n General

La Generaci√≥n Aumentada por Recuperaci√≥n (RAG, por sus siglas en ingl√©s) es uno de los patrones m√°s potentes para construir aplicaciones de IA que necesitan trabajar con tus propios datos. En lugar de depender √∫nicamente de los datos de entrenamiento de un modelo de lenguaje, los sistemas RAG recuperan informaci√≥n relevante de tus documentos y la utilizan para generar respuestas precisas y contextuales.

**Lo que construir√°s**: Un sistema RAG completo que puede responder preguntas bas√°ndose en tus propios documentos usando embeddings, almacenamiento vectorial y recuperaci√≥n LLM.

**Casos de uso**:
- Bases de conocimiento internas y b√∫squeda de documentaci√≥n
- Sistemas de soporte al cliente con informaci√≥n espec√≠fica de la empresa
- Asistentes de investigaci√≥n que trabajan con documentos de dominio espec√≠fico
- Interfaces de chat para grandes colecciones de documentos

**Tiempo para completar**: 45-60 minutos

## Prerrequisitos

**Conocimientos requeridos**:
- Python 3.9+
- Comprensi√≥n b√°sica de APIs y operaciones as√≠ncronas
- Familiaridad con entornos virtuales
- Comprensi√≥n b√°sica de c√≥mo funcionan los LLMs

**Cuentas/herramientas requeridas**:
- Clave API de OpenAI ([Cons√≠guelo aqu√≠](https://platform.openai.com/api-keys))
- Python 3.9 o superior instalado
- Git y un editor de c√≥digo (VS Code recomendado)

**Opcional pero √∫til**:
- Comprensi√≥n de embeddings vectoriales
- Experiencia con LangChain (cubriremos los conceptos b√°sicos)
- Familiaridad con Jupyter notebooks para pruebas

## Descripci√≥n de la Arquitectura

```
Consulta del Usuario ‚Üí Modelo de Embedding ‚Üí B√∫squeda Vectorial ‚Üí Recuperaci√≥n de Contexto
                                                    ‚Üì
                                Generaci√≥n LLM ‚Üê Documentos Recuperados
                                                    ‚Üì
                                                Respuesta
```

**Componentes clave**:
- **Cargador de Documentos**: Ingiere y procesa tus documentos
- **Divisor de Texto**: Divide documentos en fragmentos manejables
- **Modelo de Embedding**: Convierte texto en representaciones vectoriales
- **Almac√©n Vectorial**: Almacena y busca embeddings (usaremos ChromaDB)
- **Cadena de Recuperaci√≥n**: Orquesta el flujo de trabajo RAG
- **LLM**: Genera respuestas basadas en el contexto recuperado

## Configuraci√≥n del Entorno

### Instalar Dependencias

```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar paquetes requeridos
pip install langchain langchain-openai langchain-community chromadb tiktoken pypdf python-dotenv
```

### Configuraci√≥n

Crea un archivo `.env` en la ra√≠z de tu proyecto:

```text
OPENAI_API_KEY=tu-clave-openai-aqui
```

**Nota de seguridad**: Nunca subas archivos `.env` al control de versiones. A√±ade a `.gitignore`:

```text
# .gitignore
.env
venv/
__pycache__/
*.pyc
.DS_Store
chroma_db/
```

## Implementaci√≥n

### Paso 1: Configurando los Fundamentos

**Objetivo**: Inicializar los componentes principales y cargar variables de entorno.

Crea un archivo llamado `sistema_rag.py`:

```python
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Cargar variables de entorno
load_dotenv()

# Verificar que la clave API est√° cargada
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY no encontrada en las variables de entorno")

print("‚úÖ Entorno cargado exitosamente")
```

**Por qu√© esto funciona**: Estamos usando `python-dotenv` para cargar de forma segura las claves API desde el archivo `.env`, manteniendo la informaci√≥n sensible fuera de nuestro c√≥digo.

**Problemas comunes**:
- **Problema**: `ModuleNotFoundError: No module named 'langchain'`
  - **Soluci√≥n**: Aseg√∫rate de haber activado tu entorno virtual antes de instalar los paquetes

### Paso 2: Cargando y Procesando Documentos

**Objetivo**: Cargar tus documentos y dividirlos en fragmentos adecuados para embedding.

```python
def cargar_documentos(ruta_directorio="./documentos"):
    """
    Cargar documentos desde un directorio.
    Soporta: archivos .txt, .pdf, .md
    """
    # Cargar archivos de texto
    cargador_texto = DirectoryLoader(
        ruta_directorio,
        glob="**/*.txt",
        loader_cls=TextLoader
    )

    # Cargar archivos PDF
    cargador_pdf = DirectoryLoader(
        ruta_directorio,
        glob="**/*.pdf",
        loader_cls=PyPDFLoader
    )

    docs_texto = cargador_texto.load()
    docs_pdf = cargador_pdf.load()

    todos_docs = docs_texto + docs_pdf

    print(f"‚úÖ Cargados {len(todos_docs)} documentos")
    return todos_docs


def dividir_documentos(documentos):
    """
    Dividir documentos en fragmentos para procesamiento.
    Tama√±o de fragmento: 1000 caracteres con 200 caracteres de superposici√≥n
    """
    divisor_texto = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )

    fragmentos = divisor_texto.split_documents(documentos)
    print(f"‚úÖ Dividido en {len(fragmentos)} fragmentos")
    return fragmentos
```

**¬øPor qu√© estos par√°metros?**:
- **chunk_size=1000**: Lo suficientemente grande para mantener el contexto pero lo suficientemente peque√±o para embedding eficiente
- **chunk_overlap=200**: Asegura que la informaci√≥n importante en los l√≠mites de los fragmentos no se pierda
- **RecursiveCharacterTextSplitter**: Intenta dividir primero en p√°rrafos, luego en oraciones, luego en palabras

**Consideraciones de rendimiento**:
- Para documentaci√≥n t√©cnica, considera fragmentos m√°s grandes (1500-2000 caracteres)
- Para datos conversacionales, fragmentos m√°s peque√±os (500-800 caracteres) funcionan mejor

### Paso 3: Creando el Almac√©n Vectorial

**Objetivo**: Generar embeddings y almacenarlos en ChromaDB para recuperaci√≥n eficiente.

```python
def crear_almacen_vectorial(fragmentos, directorio_persistencia="./chroma_db"):
    """
    Crear embeddings y almacenar en ChromaDB.
    Usa el modelo text-embedding-3-small de OpenAI.
    """
    # Inicializar embeddings
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )

    # Crear almac√©n vectorial
    almacen_vectorial = Chroma.from_documents(
        documents=fragmentos,
        embedding=embeddings,
        persist_directory=directorio_persistencia
    )

    print(f"‚úÖ Creado almac√©n vectorial con {len(fragmentos)} embeddings")
    return almacen_vectorial


def cargar_almacen_vectorial_existente(directorio_persistencia="./chroma_db"):
    """
    Cargar un almac√©n vectorial existente desde disco.
    """
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )

    almacen_vectorial = Chroma(
        persist_directory=directorio_persistencia,
        embedding_function=embeddings
    )

    print("‚úÖ Cargado almac√©n vectorial existente")
    return almacen_vectorial
```

**¬øPor qu√© text-embedding-3-small?**:
- Costo-efectivo: ~$0.02 por 1M tokens
- R√°pido: Menor latencia que modelos m√°s grandes
- Calidad suficiente para la mayor√≠a de aplicaciones RAG
- Vectores de 1536 dimensiones (buen equilibrio de calidad y almacenamiento)

**Problemas comunes**:
- **Problema**: `chromadb.errors.InvalidDimensionError`
  - **Soluci√≥n**: Aseg√∫rate de estar usando el mismo modelo de embedding al cargar un almac√©n existente

### Paso 4: Construyendo la Cadena RAG

**Objetivo**: Crear el pipeline de recuperaci√≥n y generaci√≥n.

```python
def crear_cadena_rag(almacen_vectorial):
    """
    Crear una cadena RetrievalQA para responder preguntas.
    """
    # Inicializar LLM
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0  # Respuestas m√°s determin√≠sticas
    )

    # Crear plantilla de prompt personalizada
    plantilla_prompt = """Usa los siguientes fragmentos de contexto para responder la pregunta al final.
Si no conoces la respuesta bas√°ndote en el contexto, simplemente di que no lo sabes, no intentes inventar una respuesta.
Siempre cita la fuente de tu informaci√≥n cuando sea posible.

Contexto: {context}

Pregunta: {question}

Respuesta: """

    PROMPT = PromptTemplate(
        template=plantilla_prompt,
        input_variables=["context", "question"]
    )

    # Crear cadena de recuperaci√≥n
    cadena_qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # Meter todos los docs recuperados en el contexto
        retriever=almacen_vectorial.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}  # Recuperar los 4 fragmentos m√°s similares
        ),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

    print("‚úÖ Cadena RAG creada exitosamente")
    return cadena_qa
```

**Explicaciones de par√°metros**:
- **temperature=0**: Hace que las respuestas sean m√°s consistentes y factuales
- **k=4**: Recupera 4 fragmentos m√°s relevantes (ajusta seg√∫n tus necesidades)
- **chain_type="stuff"**: Enfoque simple que concatena todos los docs recuperados
- **return_source_documents=True**: Devuelve los fragmentos fuente para transparencia

**Tipos de cadena alternativos**:
- `"map_reduce"`: Mejor para manejar muchos documentos
- `"refine"`: Refina iterativamente la respuesta usando cada documento
- `"map_rerank"`: Clasifica m√∫ltiples respuestas candidatas

### Paso 5: Interfaz de Consulta

**Objetivo**: Crear una interfaz amigable para consultar el sistema RAG.

```python
def consultar_rag(cadena_qa, pregunta):
    """
    Consultar el sistema RAG y devolver resultados formateados.
    """
    resultado = cadena_qa.invoke({"query": pregunta})

    respuesta = resultado["result"]
    fuentes = resultado["source_documents"]

    print("\n" + "="*80)
    print(f"Pregunta: {pregunta}")
    print("="*80)
    print(f"\nRespuesta:\n{respuesta}\n")

    if fuentes:
        print(f"Fuentes ({len(fuentes)} documentos):")
        for i, doc in enumerate(fuentes, 1):
            print(f"\n{i}. {doc.metadata.get('source', 'Fuente desconocida')}")
            print(f"   Vista previa del contenido: {doc.page_content[:200]}...")

    print("="*80 + "\n")

    return resultado


def modo_interactivo(cadena_qa):
    """
    Modo interactivo de preguntas y respuestas.
    """
    print("\nü§ñ ¬°Sistema RAG Listo! Escribe 'salir' para terminar.\n")

    while True:
        pregunta = input("T√∫: ").strip()

        if pregunta.lower() in ['salir', 'exit', 'quit', 'q']:
            print("¬°Adi√≥s! üëã")
            break

        if not pregunta:
            continue

        consultar_rag(cadena_qa, pregunta)
```

### Paso 6: Flujo de Ejecuci√≥n Principal

**Objetivo**: Unir todo en una aplicaci√≥n completa.

```python
def main():
    """
    Flujo de ejecuci√≥n principal para el sistema RAG.
    """
    import os

    # Configuraci√≥n
    RUTA_DOCUMENTOS = "./documentos"
    RUTA_ALMACEN_VECTORIAL = "./chroma_db"

    # Verificar si existe el almac√©n vectorial
    if os.path.exists(RUTA_ALMACEN_VECTORIAL):
        print("üìÇ Cargando almac√©n vectorial existente...")
        almacen_vectorial = cargar_almacen_vectorial_existente(RUTA_ALMACEN_VECTORIAL)
    else:
        print("üìÇ Creando nuevo almac√©n vectorial...")

        # Crear directorio de documentos si no existe
        os.makedirs(RUTA_DOCUMENTOS, exist_ok=True)

        # Cargar y procesar documentos
        documentos = cargar_documentos(RUTA_DOCUMENTOS)

        if not documentos:
            print("‚ùå No se encontraron documentos en ./documentos/")
            print("Por favor, a√±ade archivos .txt o .pdf al directorio documentos")
            return

        fragmentos = dividir_documentos(documentos)
        almacen_vectorial = crear_almacen_vectorial(fragmentos, RUTA_ALMACEN_VECTORIAL)

    # Crear cadena RAG
    cadena_qa = crear_cadena_rag(almacen_vectorial)

    # Consultas de ejemplo
    print("\nüß™ Probando con consultas de ejemplo...")
    consultar_rag(cadena_qa, "¬øCu√°les son los temas principales cubiertos en estos documentos?")

    # Iniciar modo interactivo
    modo_interactivo(cadena_qa)


if __name__ == "__main__":
    main()
```

## Pruebas

### Preparar Documentos de Prueba

Crea un directorio `documentos/` y a√±ade algunos archivos de prueba:

```bash
mkdir documentos
echo "La Inteligencia Artificial est√° transformando el desarrollo de software.
Los sistemas RAG combinan el poder de la recuperaci√≥n y generaci√≥n para crear
aplicaciones de IA m√°s precisas." > documentos/intro_ia.txt

echo "LangChain es un framework para desarrollar aplicaciones impulsadas por
modelos de lenguaje. Proporciona herramientas para carga de documentos, divisi√≥n
de texto, embeddings y cadenas." > documentos/intro_langchain.txt
```

### Ejecutar el Sistema

```bash
python sistema_rag.py
```

**Salida esperada**:
```
‚úÖ Entorno cargado exitosamente
üìÇ Creando nuevo almac√©n vectorial...
‚úÖ Cargados 2 documentos
‚úÖ Dividido en 8 fragmentos
‚úÖ Creado almac√©n vectorial con 8 embeddings
‚úÖ Cadena RAG creada exitosamente

üß™ Probando con consultas de ejemplo...
================================================================================
Pregunta: ¬øCu√°les son los temas principales cubiertos en estos documentos?
================================================================================

Respuesta:
Los temas principales cubiertos incluyen el impacto de la Inteligencia Artificial
en el desarrollo de software, los sistemas RAG (Generaci√≥n Aumentada por Recuperaci√≥n),
y LangChain como framework para construir aplicaciones de modelos de lenguaje...

ü§ñ ¬°Sistema RAG Listo! Escribe 'salir' para terminar.
```

### Pruebas Unitarias

Crea un archivo llamado `test_rag.py`:

```python
import pytest
from sistema_rag import dividir_documentos, crear_almacen_vectorial
from langchain.schema import Document

def test_division_documentos():
    """Probar que los documentos se dividen correctamente"""
    docs = [Document(page_content="Este es un documento de prueba. " * 100)]
    fragmentos = dividir_documentos(docs)

    assert len(fragmentos) > 1
    assert all(len(fragmento.page_content) <= 1200 for fragmento in fragmentos)  # 1000 + superposici√≥n


def test_creacion_almacen_vectorial():
    """Probar creaci√≥n de almac√©n vectorial con datos de muestra"""
    docs_prueba = [
        Document(page_content="Los sistemas RAG son potentes"),
        Document(page_content="LangChain simplifica el desarrollo de IA")
    ]

    almacen_vectorial = crear_almacen_vectorial(docs_prueba, persist_directory="./test_chroma")

    # Probar recuperaci√≥n
    resultados = almacen_vectorial.similarity_search("RAG", k=1)
    assert len(resultados) == 1
    assert "RAG" in resultados[0].page_content


def test_relevancia_recuperacion():
    """Probar que la recuperaci√≥n devuelve documentos relevantes"""
    from sistema_rag import cargar_almacen_vectorial_existente

    almacen_vectorial = cargar_almacen_vectorial_existente("./chroma_db")

    # Consultar sobre un tema espec√≠fico
    resultados = almacen_vectorial.similarity_search("¬øQu√© es LangChain?", k=3)

    assert len(resultados) > 0
    # Al menos un resultado debe mencionar LangChain
    assert any("LangChain" in doc.page_content for doc in resultados)
```

Ejecutar pruebas:

```bash
pip install pytest
pytest test_rag.py -v
```

## Optimizaci√≥n

### Ajuste de Rendimiento

**Optimizaci√≥n de embeddings**:
```python
# Procesar por lotes grandes conjuntos de documentos
def embeddings_por_lotes(fragmentos, tamano_lote=100):
    """Procesar embeddings en lotes para evitar l√≠mites de tasa"""
    todos_embeddings = []

    for i in range(0, len(fragmentos), tamano_lote):
        lote = fragmentos[i:i+tamano_lote]
        almacen_vectorial = crear_almacen_vectorial(lote)
        todos_embeddings.extend(almacen_vectorial)

        # Peque√±o retraso para evitar l√≠mites de tasa
        import time
        time.sleep(1)

    return todos_embeddings
```

**Optimizaci√≥n de recuperaci√≥n**:
```python
# Usar MMR (M√°xima Relevancia Marginal) para resultados diversos
recuperador = almacen_vectorial.as_retriever(
    search_type="mmr",  # Resultados m√°s diversos
    search_kwargs={
        "k": 4,
        "fetch_k": 20,  # Obtener m√°s candidatos antes de MMR
        "lambda_mult": 0.5  # Balance entre relevancia y diversidad
    }
)
```

**Filtrado de metadatos**:
```python
# A√±adir metadatos al crear documentos
from langchain.schema import Document

docs_con_metadata = [
    Document(
        page_content=contenido,
        metadata={
            "source": nombre_archivo,
            "category": "tecnico",
            "date": "2025-01-15"
        }
    )
    for contenido, nombre_archivo in datos_docs
]

# Filtrar durante recuperaci√≥n
recuperador = almacen_vectorial.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {"category": "tecnico"}
    }
)
```

### Optimizaci√≥n de Costos

**Costos estimados** (a enero de 2025):
- Embeddings (text-embedding-3-small): $0.02 por 1M tokens (~$0.02 por 5000 p√°ginas)
- Llamadas LLM (gpt-4o-mini): $0.15 por 1M tokens de entrada, $0.60 por 1M tokens de salida
- ChromaDB: Gratis (almacenamiento local)

**Estrategias para ahorrar costos**:

1. **Cachear embeddings** - Solo regenerar cuando los documentos cambien
2. **Usar modelos m√°s baratos para consultas simples**:
```python
# Usar gpt-4o-mini para la mayor√≠a de consultas, gpt-4 para complejas
def obtener_llm_para_consulta(complejidad_consulta="simple"):
    if complejidad_consulta == "compleja":
        return ChatOpenAI(model="gpt-4o", temperature=0)
    return ChatOpenAI(model="gpt-4o-mini", temperature=0)
```

3. **Implementar cach√© sem√°ntico**:
```python
from langchain.cache import InMemoryCache
import langchain

langchain.llm_cache = InMemoryCache()
```

## Despliegue

### Contenedor Docker

Crea un `Dockerfile`:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copiar requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo de la aplicaci√≥n
COPY . .

# Crear directorios para documentos y almac√©n vectorial
RUN mkdir -p documentos chroma_db

# Ejecutar la aplicaci√≥n
CMD ["python", "sistema_rag.py"]
```

Crea `requirements.txt`:
```text
langchain==0.3.15
langchain-openai==0.2.14
langchain-community==0.3.15
chromadb==0.5.23
tiktoken==0.8.0
pypdf==5.1.0
python-dotenv==1.0.1
```

Construir y ejecutar:
```bash
docker build -t mi-sistema-rag .
docker run --env-file .env -v $(pwd)/documentos:/app/documentos mi-sistema-rag
```

### Consideraciones de Producci√≥n

**Monitoreo**:
```python
import time
from functools import wraps

def registrar_rendimiento(func):
    """Decorador para registrar el rendimiento de funciones"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        inicio = time.time()
        resultado = func(*args, **kwargs)
        duracion = time.time() - inicio
        print(f"{func.__name__} tom√≥ {duracion:.2f} segundos")
        return resultado
    return wrapper

@registrar_rendimiento
def consultar_rag(cadena_qa, pregunta):
    # ... c√≥digo existente
```

**Manejo de errores**:
```python
from openai import RateLimitError, APIError

def consultar_con_reintentos(cadena_qa, pregunta, max_reintentos=3):
    """Consultar con reintento autom√°tico en l√≠mites de tasa"""
    for intento in range(max_reintentos):
        try:
            return cadena_qa.invoke({"query": pregunta})
        except RateLimitError:
            if intento < max_reintentos - 1:
                tiempo_espera = 2 ** intento
                print(f"L√≠mite de tasa alcanzado. Esperando {tiempo_espera}s...")
                time.sleep(tiempo_espera)
            else:
                raise
        except APIError as e:
            print(f"Error de API: {e}")
            raise
```

**Lista de verificaci√≥n de seguridad**:
- [ ] Claves API almacenadas en variables de entorno, nunca en el c√≥digo
- [ ] Validaci√≥n de entrada para prevenir ataques de inyecci√≥n
- [ ] L√≠mite de tasa implementado
- [ ] Autenticaci√≥n de usuario si se expone como servicio
- [ ] Registro de consultas para auditor√≠as
- [ ] Rotaci√≥n regular de claves API

## Soluci√≥n de Problemas

### Errores Comunes

**Error**: `RateLimitError: Rate limit exceeded for text-embedding-3-small`
```python
# Soluci√≥n: Implementar retroceso exponencial
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def crear_embeddings_con_reintentos(fragmentos):
    return crear_almacen_vectorial(fragmentos)
```

**Error**: `ChromaDB database is locked`
- **Causa**: M√∫ltiples procesos accediendo a la misma base de datos
- **Soluci√≥n**: Asegurar que solo un proceso accede a ChromaDB a la vez, o usar configuraci√≥n cliente-servidor:

```python
import chromadb
from chromadb.config import Settings

# Modo cliente-servidor
client = chromadb.Client(Settings(
    chroma_api_impl="rest",
    chroma_server_host="localhost",
    chroma_server_http_port="8000"
))
```

**Error**: `No se devuelven resultados de la recuperaci√≥n`
- **Causa**: Los t√©rminos de consulta no coinciden bien con el contenido del documento
- **Soluci√≥n**:
  1. Verificar si los documentos fueron realmente embebidos
  2. Intentar t√©rminos de consulta m√°s gen√©ricos
  3. Ajustar umbral de similitud
  4. Usar b√∫squeda h√≠brida (sem√°ntica + palabra clave)

**Error**: `Sin memoria al procesar documentos grandes`
- **Soluci√≥n**: Procesar documentos en lotes y usar streaming:
```python
def procesar_documento_grande(ruta_archivo, tamano_fragmento=1000):
    """Procesar por streaming documentos grandes"""
    with open(ruta_archivo, 'r') as f:
        buffer = ""
        for linea in f:
            buffer += linea
            if len(buffer) >= tamano_fragmento:
                yield buffer
                buffer = ""
        if buffer:
            yield buffer
```

## Pr√≥ximos Pasos

**Mejoras a considerar**:
- [ ] A√±adir memoria de conversaci√≥n para di√°logos multi-turno
- [ ] Implementar b√∫squeda h√≠brida (BM25 + sem√°ntica)
- [ ] A√±adir re-ranking con un modelo cross-encoder
- [ ] Crear una interfaz web con Streamlit o Gradio
- [ ] A√±adir soporte para m√°s tipos de documentos (CSV, JSON, HTML)
- [ ] Implementar recopilaci√≥n de feedback del usuario para mejorar la recuperaci√≥n
- [ ] A√±adir m√©tricas de evaluaci√≥n (precisi√≥n de recuperaci√≥n, calidad de respuestas)
- [ ] Configurar observabilidad con LangSmith o Weights & Biases

**Gu√≠as relacionadas**:
- [Arquitectura de Agentes de IA: Patrones y Mejores Pr√°cticas](/es/developers/arquitectura-agentes-patrones)
- [Vectorizaci√≥n y B√∫squeda Sem√°ntica: Gu√≠a Completa](/es/developers/vectorizacion-busqueda-semantica)
- [Prompt Engineering para Desarrolladores](/es/developers/prompt-engineering-desarrolladores)

## Recursos Adicionales

**Documentaci√≥n oficial**:
- [Documentaci√≥n de LangChain](https://python.langchain.com/docs/get_started/introduction)
- [Referencia API de OpenAI](https://platform.openai.com/docs/api-reference)
- [Documentaci√≥n de ChromaDB](https://docs.trychroma.com/)

**Temas avanzados**:
- [RAG from Scratch](https://github.com/langchain-ai/rag-from-scratch) - Serie de videos de LangChain
- [T√©cnicas Avanzadas de RAG](https://blog.langchain.dev/deconstructing-rag/) - Blog de LangChain
- [Inmersi√≥n Profunda en Embeddings](https://platform.openai.com/docs/guides/embeddings) - Gu√≠a de OpenAI

**Comunidad**:
- [Discord de LangChain](https://discord.gg/langchain)
- [r/LangChain](https://reddit.com/r/LangChain)
- [Discusiones de GitHub de LangChain](https://github.com/langchain-ai/langchain/discussions)

**Repositorios de ejemplo**:
- [Ejemplos RAG de LangChain](https://github.com/langchain-ai/langchain/tree/master/templates)
- [Plantilla RAG de Producci√≥n](https://github.com/langchain-ai/rag-template)

---

**¬øEncontraste un problema con esta gu√≠a?** ¬°[Abre un issue](https://github.com/javirub/The-New-Era-Codex/issues) o env√≠a un PR!
