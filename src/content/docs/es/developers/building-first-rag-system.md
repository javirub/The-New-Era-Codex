---
title: "Construyendo tu Primer Sistema RAG con LangChain"
description: "Tutorial paso a paso para crear un sistema RAG b√°sico con embeddings, ChromaDB y OpenAI"
sidebar:
  badge:
    text: "Intermedio"
    variant: note
version: "1.0"
---

# Construyendo tu Primer Sistema RAG con LangChain

## Descripci√≥n General

La Generaci√≥n Aumentada por Recuperaci√≥n (RAG) es uno de los patrones m√°s poderosos para construir aplicaciones de IA que necesitan trabajar con tus propios datos. En lugar de depender √∫nicamente de los datos de entrenamiento de un modelo de lenguaje, los sistemas RAG recuperan informaci√≥n relevante de tus documentos y la usan para generar respuestas precisas y contextuales.

**Lo que construir√°s**: Un sistema RAG completo que puede responder preguntas basadas en tus propios documentos usando embeddings, almacenamiento vectorial y recuperaci√≥n LLM.

**Casos de uso**:
- Bases de conocimiento internas y b√∫squeda de documentaci√≥n
- Sistemas de soporte al cliente con informaci√≥n espec√≠fica de la empresa
- Asistentes de investigaci√≥n que trabajan con documentos espec√≠ficos de dominio
- Interfaces de chat para grandes colecciones de documentos

**Tiempo para completar**: 45-60 minutos

## Prerrequisitos

**Conocimientos requeridos**:
- Python 3.9+
- Comprensi√≥n b√°sica de APIs y operaciones as√≠ncronas
- Familiaridad con entornos virtuales
- Comprensi√≥n b√°sica de c√≥mo funcionan los LLMs

**Cuentas/herramientas requeridas**:
- Clave API de OpenAI ([Obt√©n una aqu√≠](https://platform.openai.com/api-keys))
- Python 3.9 o superior instalado
- Git y un editor de c√≥digo (VS Code recomendado)

**Opcional pero √∫til**:
- Comprensi√≥n de embeddings vectoriales
- Experiencia con LangChain (cubriremos lo b√°sico)
- Familiaridad con notebooks de Jupyter para pruebas

## Descripci√≥n de la Arquitectura

```
Consulta del Usuario ‚Üí Modelo de Embedding ‚Üí B√∫squeda Vectorial ‚Üí Recuperaci√≥n de Contexto
                                      ‚Üì
                                Generaci√≥n LLM ‚Üê Documentos Recuperados
                                      ‚Üì
                                  Respuesta
```

**Componentes clave**:
- **Cargador de Documentos**: Ingiere y procesa tus documentos
- **Divisor de Texto**: Divide documentos en fragmentos manejables
- **Modelo de Embedding**: Convierte texto en representaciones vectoriales
- **Almac√©n Vectorial**: Almacena y busca embeddings (usaremos ChromaDB)
- **Cadena de Recuperaci√≥n**: Orquesta el flujo de trabajo RAG
- **LLM**: Genera respuestas basadas en el contexto recuperado

## Configuraci√≥n del Entorno

### Instalaci√≥n de Dependencias

```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar paquetes requeridos
pip install langchain langchain-openai langchain-community chromadb tiktoken pypdf python-dotenv
```

### Configuraci√≥n

Crea un archivo `.env` en la ra√≠z de tu proyecto:

```text
OPENAI_API_KEY=tu-clave-openai-aqu√≠
```

**Nota de seguridad**: Nunca hagas commit de archivos `.env` al control de versiones. Agr√©galo a `.gitignore`:

```text
# .gitignore
.env
venv/
__pycache__/
*.pyc
.DS_Store
chroma_db/
```

## Implementaci√≥n

### Paso 1: Configuraci√≥n de la Base

**Objetivo**: Inicializar los componentes principales y cargar las variables de entorno.

Crea un archivo llamado `rag_system.py`:

```python
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Load environment variables
load_dotenv()

# Verify API key is loaded
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY not found in environment variables")

print("‚úÖ Environment loaded successfully")
```

**Por qu√© esto funciona**: Estamos usando `python-dotenv` para cargar de forma segura las claves API desde el archivo `.env`, manteniendo la informaci√≥n sensible fuera de nuestro c√≥digo.

**Problemas comunes**:
- **Problema**: `ModuleNotFoundError: No module named 'langchain'`
  - **Soluci√≥n**: Aseg√∫rate de haber activado tu entorno virtual antes de instalar los paquetes

### Paso 2: Cargar y Procesar Documentos

**Objetivo**: Cargar tus documentos y dividirlos en fragmentos adecuados para embedding.

```python
def load_documents(directory_path="./documents"):
    """
    Load documents from a directory.
    Supports: .txt, .pdf, .md files
    """
    # Load text files
    text_loader = DirectoryLoader(
        directory_path,
        glob="**/*.txt",
        loader_cls=TextLoader
    )

    # Load PDF files
    pdf_loader = DirectoryLoader(
        directory_path,
        glob="**/*.pdf",
        loader_cls=PyPDFLoader
    )

    text_docs = text_loader.load()
    pdf_docs = pdf_loader.load()

    all_docs = text_docs + pdf_docs

    print(f"‚úÖ Loaded {len(all_docs)} documents")
    return all_docs


def split_documents(documents):
    """
    Split documents into chunks for processing.
    Chunk size: 1000 characters with 200 character overlap
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )

    chunks = text_splitter.split_documents(documents)
    print(f"‚úÖ Split into {len(chunks)} chunks")
    return chunks
```

**¬øPor qu√© estos par√°metros?**:
- **chunk_size=1000**: Lo suficientemente grande para mantener el contexto pero lo suficientemente peque√±o para embedding eficiente
- **chunk_overlap=200**: Asegura que la informaci√≥n importante en los l√≠mites de los fragmentos no se pierda
- **RecursiveCharacterTextSplitter**: Intenta dividir primero en p√°rrafos, luego en oraciones, luego en palabras

**Consideraciones de rendimiento**:
- Para documentaci√≥n t√©cnica, considera fragmentos m√°s grandes (1500-2000 caracteres)
- Para datos conversacionales, fragmentos m√°s peque√±os (500-800 caracteres) funcionan mejor

### Paso 3: Creaci√≥n del Almac√©n Vectorial

**Objetivo**: Generar embeddings y almacenarlos en ChromaDB para recuperaci√≥n eficiente.

```python
def create_vector_store(chunks, persist_directory="./chroma_db"):
    """
    Create embeddings and store in ChromaDB.
    Uses OpenAI's text-embedding-3-small model.
    """
    # Initialize embeddings
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )

    # Create vector store
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )

    print(f"‚úÖ Created vector store with {len(chunks)} embeddings")
    return vectorstore


def load_existing_vector_store(persist_directory="./chroma_db"):
    """
    Load an existing vector store from disk.
    """
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )

    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )

    print("‚úÖ Loaded existing vector store")
    return vectorstore
```

**¬øPor qu√© text-embedding-3-small?**:
- Rentable: ~$0.02 por 1M tokens
- R√°pido: Menor latencia que modelos m√°s grandes
- Calidad suficiente para la mayor√≠a de aplicaciones RAG
- Vectores de 1536 dimensiones (buen equilibrio de calidad y almacenamiento)

**Problemas comunes**:
- **Problema**: `chromadb.errors.InvalidDimensionError`
  - **Soluci√≥n**: Aseg√∫rate de usar el mismo modelo de embedding al cargar un almac√©n existente

### Paso 4: Construcci√≥n de la Cadena RAG

**Objetivo**: Crear el pipeline de recuperaci√≥n y generaci√≥n.

```python
def create_rag_chain(vectorstore):
    """
    Create a RetrievalQA chain for question answering.
    """
    # Initialize LLM
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0  # More deterministic responses
    )

    # Create custom prompt template
    prompt_template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer based on the context, just say that you don't know, don't try to make up an answer.
Always cite the source of your information when possible.

Context: {context}

Question: {question}

Answer: """

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # Create retrieval chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # Stuff all retrieved docs into context
        retriever=vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}  # Retrieve top 4 most similar chunks
        ),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

    print("‚úÖ RAG chain created successfully")
    return qa_chain
```

**Explicaci√≥n de par√°metros**:
- **temperature=0**: Hace las respuestas m√°s consistentes y factuales
- **k=4**: Recupera los 4 fragmentos m√°s relevantes (ajusta seg√∫n tus necesidades)
- **chain_type="stuff"**: Enfoque simple que concatena todos los documentos recuperados
- **return_source_documents=True**: Devuelve los fragmentos fuente para transparencia

**Tipos de cadena alternativos**:
- `"map_reduce"`: Mejor para manejar muchos documentos
- `"refine"`: Refina iterativamente la respuesta usando cada documento
- `"map_rerank"`: Clasifica m√∫ltiples respuestas candidatas

### Paso 5: Interfaz de Consulta

**Objetivo**: Crear una interfaz amigable para consultar el sistema RAG.

```python
def query_rag(qa_chain, question):
    """
    Query the RAG system and return formatted results.
    """
    result = qa_chain.invoke({"query": question})

    answer = result["result"]
    sources = result["source_documents"]

    print("\n" + "="*80)
    print(f"Question: {question}")
    print("="*80)
    print(f"\nAnswer:\n{answer}\n")

    if sources:
        print(f"Sources ({len(sources)} documents):")
        for i, doc in enumerate(sources, 1):
            print(f"\n{i}. {doc.metadata.get('source', 'Unknown source')}")
            print(f"   Content preview: {doc.page_content[:200]}...")

    print("="*80 + "\n")

    return result


def interactive_mode(qa_chain):
    """
    Interactive question-answering mode.
    """
    print("\nü§ñ RAG System Ready! Type 'exit' to quit.\n")

    while True:
        question = input("You: ").strip()

        if question.lower() in ['exit', 'quit', 'q']:
            print("Goodbye! üëã")
            break

        if not question:
            continue

        query_rag(qa_chain, question)
```

### Paso 6: Flujo de Ejecuci√≥n Principal

**Objetivo**: Unir todo en una aplicaci√≥n completa.

```python
def main():
    """
    Main execution flow for the RAG system.
    """
    import os

    # Configuration
    DOCUMENTS_PATH = "./documents"
    VECTOR_STORE_PATH = "./chroma_db"

    # Check if vector store exists
    if os.path.exists(VECTOR_STORE_PATH):
        print("üìÇ Loading existing vector store...")
        vectorstore = load_existing_vector_store(VECTOR_STORE_PATH)
    else:
        print("üìÇ Creating new vector store...")

        # Create documents directory if it doesn't exist
        os.makedirs(DOCUMENTS_PATH, exist_ok=True)

        # Load and process documents
        documents = load_documents(DOCUMENTS_PATH)

        if not documents:
            print("‚ùå No documents found in ./documents/")
            print("Please add .txt or .pdf files to the documents directory")
            return

        chunks = split_documents(documents)
        vectorstore = create_vector_store(chunks, VECTOR_STORE_PATH)

    # Create RAG chain
    qa_chain = create_rag_chain(vectorstore)

    # Example queries
    print("\nüß™ Testing with example queries...")
    query_rag(qa_chain, "What are the main topics covered in these documents?")

    # Start interactive mode
    interactive_mode(qa_chain)


if __name__ == "__main__":
    main()
```

## Pruebas

### Preparar Documentos de Prueba

Crea un directorio `documents/` y agrega algunos archivos de prueba:

```bash
mkdir documents
echo "Artificial Intelligence is transforming software development.
RAG systems combine the power of retrieval and generation to create
more accurate AI applications." > documents/ai_intro.txt

echo "LangChain is a framework for developing applications powered by
language models. It provides tools for document loading, text splitting,
embeddings, and chains." > documents/langchain_intro.txt
```

### Ejecutar el Sistema

```bash
python rag_system.py
```

**Salida esperada**:
```
‚úÖ Environment loaded successfully
üìÇ Creating new vector store...
‚úÖ Loaded 2 documents
‚úÖ Split into 8 chunks
‚úÖ Created vector store with 8 embeddings
‚úÖ RAG chain created successfully

üß™ Testing with example queries...
================================================================================
Question: What are the main topics covered in these documents?
================================================================================

Answer:
The main topics covered include Artificial Intelligence's impact on software
development, RAG (Retrieval-Augmented Generation) systems, and LangChain as a
framework for building language model applications...

ü§ñ RAG System Ready! Type 'exit' to quit.
```

### Pruebas Unitarias

Crea un archivo llamado `test_rag.py`:

```python
import pytest
from rag_system import split_documents, create_vector_store
from langchain.schema import Document

def test_document_splitting():
    """Test that documents are split correctly"""
    docs = [Document(page_content="This is a test document. " * 100)]
    chunks = split_documents(docs)

    assert len(chunks) > 1
    assert all(len(chunk.page_content) <= 1200 for chunk in chunks)  # 1000 + overlap


def test_vector_store_creation():
    """Test vector store creation with sample data"""
    test_docs = [
        Document(page_content="RAG systems are powerful"),
        Document(page_content="LangChain simplifies AI development")
    ]

    vectorstore = create_vector_store(test_docs, persist_directory="./test_chroma")

    # Test retrieval
    results = vectorstore.similarity_search("RAG", k=1)
    assert len(results) == 1
    assert "RAG" in results[0].page_content


def test_retrieval_relevance():
    """Test that retrieval returns relevant documents"""
    from rag_system import load_existing_vector_store

    vectorstore = load_existing_vector_store("./chroma_db")

    # Query about a specific topic
    results = vectorstore.similarity_search("What is LangChain?", k=3)

    assert len(results) > 0
    # At least one result should mention LangChain
    assert any("LangChain" in doc.page_content for doc in results)
```

Ejecutar pruebas:

```bash
pip install pytest
pytest test_rag.py -v
```

## Optimizaci√≥n

### Ajuste de Rendimiento

**Optimizaci√≥n de embeddings**:
```python
# Batch process large document sets
def batch_embed_documents(chunks, batch_size=100):
    """Process embeddings in batches to avoid rate limits"""
    all_embeddings = []

    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        vectorstore = create_vector_store(batch)
        all_embeddings.extend(vectorstore)

        # Small delay to avoid rate limiting
        import time
        time.sleep(1)

    return all_embeddings
```

**Optimizaci√≥n de recuperaci√≥n**:
```python
# Use MMR (Maximal Marginal Relevance) for diverse results
retriever = vectorstore.as_retriever(
    search_type="mmr",  # More diverse results
    search_kwargs={
        "k": 4,
        "fetch_k": 20,  # Fetch more candidates before MMR
        "lambda_mult": 0.5  # Balance between relevance and diversity
    }
)
```

**Filtrado de metadatos**:
```python
# Add metadata when creating documents
from langchain.schema import Document

docs_with_metadata = [
    Document(
        page_content=content,
        metadata={
            "source": filename,
            "category": "technical",
            "date": "2025-01-15"
        }
    )
    for content, filename in doc_data
]

# Filter during retrieval
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {"category": "technical"}
    }
)
```

### Optimizaci√≥n de Costos

**Costos estimados** (a enero de 2025):
- Embeddings (text-embedding-3-small): $0.02 por 1M tokens (~$0.02 por 5000 p√°ginas)
- Llamadas LLM (gpt-4o-mini): $0.15 por 1M tokens de entrada, $0.60 por 1M tokens de salida
- ChromaDB: Gratis (almacenamiento local)

**Estrategias de ahorro de costos**:

1. **Cach√© de embeddings** - Solo regenerar cuando los documentos cambien
2. **Usar modelos m√°s baratos para consultas simples**:
```python
# Use gpt-4o-mini for most queries, gpt-4 for complex ones
def get_llm_for_query(query_complexity="simple"):
    if query_complexity == "complex":
        return ChatOpenAI(model="gpt-4o", temperature=0)
    return ChatOpenAI(model="gpt-4o-mini", temperature=0)
```

3. **Implementar cach√© sem√°ntico**:
```python
from langchain.cache import InMemoryCache
import langchain

langchain.llm_cache = InMemoryCache()
```

## Despliegue

### Contenedor Docker

Crea un `Dockerfile`:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create directories for documents and vector store
RUN mkdir -p documents chroma_db

# Run the application
CMD ["python", "rag_system.py"]
```

Crea `requirements.txt`:
```text
langchain==0.3.15
langchain-openai==0.2.14
langchain-community==0.3.15
chromadb==0.5.23
tiktoken==0.8.0
pypdf==5.1.0
python-dotenv==1.0.1
```

Construir y ejecutar:
```bash
docker build -t my-rag-system .
docker run --env-file .env -v $(pwd)/documents:/app/documents my-rag-system
```

### Consideraciones de Producci√≥n

**Monitoreo**:
```python
import time
from functools import wraps

def log_performance(func):
    """Decorator to log function performance"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start
        print(f"{func.__name__} took {duration:.2f} seconds")
        return result
    return wrapper

@log_performance
def query_rag(qa_chain, question):
    # ... existing code
```

**Manejo de errores**:
```python
from openai import RateLimitError, APIError

def query_with_retry(qa_chain, question, max_retries=3):
    """Query with automatic retry on rate limits"""
    for attempt in range(max_retries):
        try:
            return qa_chain.invoke({"query": question})
        except RateLimitError:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                print(f"Rate limit hit. Waiting {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise
        except APIError as e:
            print(f"API Error: {e}")
            raise
```

**Lista de verificaci√≥n de seguridad**:
- [ ] Claves API almacenadas en variables de entorno, nunca en c√≥digo
- [ ] Validaci√≥n de entrada para prevenir ataques de inyecci√≥n
- [ ] Limitaci√≥n de tasa implementada
- [ ] Autenticaci√≥n de usuario si se expone como servicio
- [ ] Registro de consultas para auditor√≠a
- [ ] Rotaci√≥n regular de claves API

## Soluci√≥n de Problemas

### Errores Comunes

**Error**: `RateLimitError: Rate limit exceeded for text-embedding-3-small`
```python
# Solution: Implement exponential backoff
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def create_embeddings_with_retry(chunks):
    return create_vector_store(chunks)
```

**Error**: `ChromaDB database is locked`
- **Causa**: M√∫ltiples procesos accediendo a la misma base de datos
- **Soluci√≥n**: Aseg√∫rate de que solo un proceso acceda a ChromaDB a la vez, o usa una configuraci√≥n cliente-servidor:

```python
import chromadb
from chromadb.config import Settings

# Client-server mode
client = chromadb.Client(Settings(
    chroma_api_impl="rest",
    chroma_server_host="localhost",
    chroma_server_http_port="8000"
))
```

**Error**: `No results returned from retrieval`
- **Causa**: Los t√©rminos de consulta no coinciden bien con el contenido del documento
- **Soluci√≥n**:
  1. Verificar si los documentos fueron realmente embebidos
  2. Intentar t√©rminos de consulta m√°s gen√©ricos
  3. Ajustar el umbral de similitud
  4. Usar b√∫squeda h√≠brida (sem√°ntica + palabra clave)

**Error**: `Out of memory when processing large documents`
- **Soluci√≥n**: Procesar documentos en lotes y usar streaming:
```python
def process_large_document(file_path, chunk_size=1000):
    """Stream process large documents"""
    with open(file_path, 'r') as f:
        buffer = ""
        for line in f:
            buffer += line
            if len(buffer) >= chunk_size:
                yield buffer
                buffer = ""
        if buffer:
            yield buffer
```

## Pr√≥ximos Pasos

**Mejoras a considerar**:
- [ ] Agregar memoria de conversaci√≥n para di√°logos multi-turno
- [ ] Implementar b√∫squeda h√≠brida (BM25 + sem√°ntica)
- [ ] Agregar re-ranking con un modelo cross-encoder
- [ ] Crear una interfaz web con Streamlit o Gradio
- [ ] Agregar soporte para m√°s tipos de documentos (CSV, JSON, HTML)
- [ ] Implementar recopilaci√≥n de retroalimentaci√≥n de usuarios para mejorar la recuperaci√≥n
- [ ] Agregar m√©tricas de evaluaci√≥n (precisi√≥n de recuperaci√≥n, calidad de respuesta)
- [ ] Configurar observabilidad con LangSmith o Weights & Biases

**Gu√≠as relacionadas**:
- [Arquitectura de Agentes IA: Patrones y Mejores Pr√°cticas](/developers/agent-architecture-patterns)
- [Vectorizaci√≥n y B√∫squeda Sem√°ntica: Gu√≠a Completa](/developers/vectorization-semantic-search)
- [Ingenier√≠a de Prompts para Desarrolladores](/developers/prompt-engineering-developers)

## Recursos Adicionales

**Documentaci√≥n oficial**:
- [Documentaci√≥n de LangChain](https://python.langchain.com/docs/get_started/introduction)
- [Referencia API de OpenAI](https://platform.openai.com/docs/api-reference)
- [Documentaci√≥n de ChromaDB](https://docs.trychroma.com/)

**Temas avanzados**:
- [RAG from Scratch](https://github.com/langchain-ai/rag-from-scratch) - Serie de videos de LangChain
- [T√©cnicas RAG Avanzadas](https://blog.langchain.dev/deconstructing-rag/) - Blog de LangChain
- [Inmersi√≥n Profunda en Embeddings](https://platform.openai.com/docs/guides/embeddings) - Gu√≠a de OpenAI

**Comunidad**:
- [Discord de LangChain](https://discord.gg/langchain)
- [r/LangChain](https://reddit.com/r/LangChain)
- [Discusiones de GitHub de LangChain](https://github.com/langchain-ai/langchain/discussions)

**Repositorios de ejemplo**:
- [Ejemplos RAG de LangChain](https://github.com/langchain-ai/langchain/tree/master/templates)
- [Plantilla RAG de Producci√≥n](https://github.com/langchain-ai/rag-template)

---

**¬øEncontraste un problema con esta gu√≠a?** [Abre un issue](https://github.com/javirub/The-New-Era-Codex/issues) o env√≠a un PR!
